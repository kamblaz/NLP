{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%run functions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading:  99%|█████████████████████████████████████████████████████▋| 11311188/11368252 [00:19<00:00, 607167.44bytes/s]"
     ]
    }
   ],
   "source": [
    "stemmer = StempelStemmer.polimorf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading: 100%|██████████████████████████████████████████████████████| 11368252/11368252 [00:20<00:00, 553535.14bytes/s]\n"
     ]
    }
   ],
   "source": [
    "xall,yall = loadDataFromCSV('data.csv')\n",
    "number_of_classes = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "xall,yall=deleteBasedOnIndicesList(xall,yall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set size:  29423\n"
     ]
    }
   ],
   "source": [
    "print(\"Set size: \", len(xall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(yall)):\n",
    "    yall[i] = f(float(yall[i]))\n",
    "for i in range(0,len(xall)):\n",
    "    xall[i] = str(xall[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_samples_for_each_class = count_number_of_samples_for_each_class(yall)\n",
    "weights = assign_weight_for_each_class(len(yall), number_of_samples_for_each_class)\n",
    "class_weight = {0: weights[0],\n",
    "                1: weights[1],\n",
    "                2: weights[2],\n",
    "                3: weights[3],\n",
    "                4: weights[4],\n",
    "                5: weights[5],\n",
    "                6: weights[6],\n",
    "                7: weights[7],\n",
    "                8: weights[8],\n",
    "                9: weights[9],\n",
    "                10: weights[10],\n",
    "                11: weights[11],\n",
    "                12: weights[12],\n",
    "                13: weights[13],\n",
    "                14: weights[14],\n",
    "                15: weights[15],\n",
    "                16: weights[16],\n",
    "                17: weights[17],\n",
    "                18: weights[18],\n",
    "                19: weights[19],\n",
    "                20: weights[20],\n",
    "                21: weights[21],\n",
    "                22: weights[22],\n",
    "                23: weights[23],\n",
    "                24: weights[24],\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1625,\n",
       " 434,\n",
       " 132,\n",
       " 847,\n",
       " 141,\n",
       " 861,\n",
       " 1630,\n",
       " 1183,\n",
       " 178,\n",
       " 1935,\n",
       " 165,\n",
       " 2000,\n",
       " 2000,\n",
       " 2000,\n",
       " 320,\n",
       " 2001,\n",
       " 671,\n",
       " 1240,\n",
       " 2003,\n",
       " 2002,\n",
       " 18,\n",
       " 2002,\n",
       " 39,\n",
       " 1996,\n",
       " 2000]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_samples_for_each_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "yall=to_categorical(yall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "porządku polecam\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('polish'))\n",
    "processed_reviews = []\n",
    "single_review = \"string to iniialize <br /> my email id is charilie@waoow.com. You can also reach to me at charlie's\"\n",
    "\n",
    "for review in range(0,len(xall)):\n",
    "    post_stemming = []\n",
    "    single_review = xall[review]\n",
    "    single_review = single_review.lower()\n",
    "    single_review = re.sub('<.*?>',' ',single_review)\n",
    "    single_review = re.sub('\\W',' ',single_review)\n",
    "    single_review = re.sub('\\s+[a-zA-Z]\\s+',' ', single_review)\n",
    "    single_review = re.sub('\\s+',' ', single_review)       \n",
    "    word_tokens = word_tokenize(single_review)    \n",
    "    filtered_sentence = \" \".join([w for w in word_tokens if w not in stop_words])\n",
    "    arr = Convert(filtered_sentence)\n",
    "    if(arr[0] or len(arr)>1):\n",
    "        for word in arr:\n",
    "            post_stemming.append(stemmer.stem(word))\n",
    "    single_review = listToString(post_stemming)    \n",
    "    processed_reviews.append(filtered_sentence)    \n",
    "print(processed_reviews[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-set size:  23538\n",
      "Test-set size:   5885\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(processed_reviews,yall,test_size=0.2,random_state=42)\n",
    "print(\"Train-set size: \", len(x_train))\n",
    "print(\"Test-set size:  \", len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text = x_train + x_test\n",
    "num_words = 1000\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "tokenizer.fit_on_texts(data_text)\n",
    "tokenizer.word_index\n",
    "x_train_tokens = tokenizer.texts_to_sequences(x_train)\n",
    "x_test_tokens = tokenizer.texts_to_sequences(x_test)\n",
    "num_tokens = [len(tokens) for tokens in x_train_tokens + x_test_tokens]\n",
    "num_tokens = np.array(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "max_tokens = int(max_tokens)\n",
    "max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad = 'pre'\n",
    "x_train_pad = pad_sequences(x_train_tokens, maxlen=max_tokens,\n",
    "                            padding=pad, truncating=pad)\n",
    "\n",
    "x_test_pad = pad_sequences(x_test_tokens, maxlen=max_tokens,\n",
    "                           padding=pad, truncating=pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = tokenizer.word_index\n",
    "inverse_map = dict(zip(idx.values(), idx.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 64)          64000     \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 16)                4672      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 25)                225       \n",
      "=================================================================\n",
      "Total params: 69,033\n",
      "Trainable params: 69,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(1000,64),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_size)),\n",
    "    tf.keras.layers.Dense(embedding_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(number_of_classes, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train on 22361 samples, validate on 1177 samples\n",
      "Epoch 1/8\n",
      "22361/22361 [==============================] - 33s 1ms/sample - loss: 2.9998 - acc: 0.0983 - val_loss: 3.2904 - val_acc: 0.1393\n",
      "Epoch 2/8\n",
      "22361/22361 [==============================] - 26s 1ms/sample - loss: 2.7783 - acc: 0.1285 - val_loss: 3.1913 - val_acc: 0.1198\n",
      "Epoch 3/8\n",
      "22361/22361 [==============================] - 27s 1ms/sample - loss: 2.6989 - acc: 0.1381 - val_loss: 3.1601 - val_acc: 0.1504\n",
      "Epoch 4/8\n",
      "22361/22361 [==============================] - 26s 1ms/sample - loss: 2.6344 - acc: 0.1389 - val_loss: 3.1103 - val_acc: 0.1003\n",
      "Epoch 5/8\n",
      "22361/22361 [==============================] - 28s 1ms/sample - loss: 2.5688 - acc: 0.1340 - val_loss: 3.1125 - val_acc: 0.1003\n",
      "Epoch 6/8\n",
      "22361/22361 [==============================] - 35s 2ms/sample - loss: 2.5179 - acc: 0.1356 - val_loss: 3.1959 - val_acc: 0.0943\n",
      "Epoch 7/8\n",
      "22361/22361 [==============================] - 33s 1ms/sample - loss: 2.4657 - acc: 0.1328 - val_loss: 3.2271 - val_acc: 0.10204s - los\n",
      "Epoch 8/8\n",
      "22361/22361 [==============================] - 35s 2ms/sample - loss: 2.4212 - acc: 0.1323 - val_loss: 3.2018 - val_acc: 0.0960\n",
      "Wall time: 4min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_train=np.array(y_train)\n",
    "model.fit(x_train_pad, y_train, class_weight=class_weight, validation_split=0.05, epochs=8, batch_size=number_of_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.57 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred = model.predict(x=x_test_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classifyRate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-3b8c3399b843>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m##pred\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mrate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclassifyRate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0my_res\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'classifyRate' is not defined"
     ]
    }
   ],
   "source": [
    "y_res=[]\n",
    "y_test_res=[]\n",
    "##pred\n",
    "for i in range(0,len(y_pred)):\n",
    "    rate=classifyRate(y_pred[i])\n",
    "    y_res.append(rate)\n",
    "\n",
    "for i in range(0,len(y_test)):\n",
    "    rate=np.argmax(y_test[i])\n",
    "    y_test_res.append(rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyRate(rate):\n",
    "    classifiedRate = np.argmax(rate)\n",
    "    rate2 = rate.copy()\n",
    "    maks = max(rate)\n",
    "    rate2 = rate2[rate2 != maks]\n",
    "    maks2 = max(rate2)\n",
    "    maksIndex = np.where(rate == maks)[0][0]\n",
    "    maksIndex2 = np.where(rate == maks2)[0][0] \n",
    "    result = -1\n",
    "    temp = findBestRate(rate, maksIndex)\n",
    "    temp2 = findBestRate(rate, maksIndex2)\n",
    "    if (temp>temp2):\n",
    "        result = maksIndex\n",
    "    else:\n",
    "        result = maksIndex2\n",
    "    #if (i>0 and i < len(rate)-1):\n",
    "     #   temp = rate[i] + 0.5*rate[i-1] + 0.5*rate[i+1]\n",
    "    #elif (i == 0):\n",
    "    #    temp = rate[i] + 0.5*rate[i+1]\n",
    "    #else:\n",
    "    #    temp = rate[i] + 0.5*rate [i-1]\n",
    "    #if (temp>maks):\n",
    "    #    maks = temp\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findBestRate(rate, maksIndex):\n",
    "    if (maksIndex>0 and maksIndex<len(rate)-1):\n",
    "        temp = rate[maksIndex]+0.5*rate[maksIndex-1]+0.5*rate[maksIndex+1]\n",
    "    elif (maksIndex == 0):\n",
    "        temp = rate[maksIndex] + 0.5*rate[maksIndex+1]\n",
    "    else:\n",
    "        temp = rate[maksIndex] + 0.5*rate[maksIndex-1]\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyRate(rate):\n",
    "    classifiedRate = np.argmax(rate)\n",
    "    maks = 0\n",
    "    maks2 = 0\n",
    "    temp = 0\n",
    "    maxIndex = 0\n",
    "    for i in range (0, len(rate)):        \n",
    "        if (i>0 and i < len(rate)-1):\n",
    "            temp = rate[i] + 0.5*rate[i-1] + 0.5*rate[i+1]\n",
    "        elif (i == 0):\n",
    "            temp = rate[i] + 0.5*rate[i+1]\n",
    "        else:\n",
    "            temp = rate[i] + 0.5*rate [i-1]\n",
    "        if (temp>maks):\n",
    "            maks = temp\n",
    "            maxIndex = i\n",
    "    return maxIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "cm=confusion_matrix(y_test_res,y_res)\n",
    "cm_display = ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm=confusion_matrix(y_test_res,y_res)\n",
    "cm_display = ConfusionMatrixDisplay(cm).plot()\n",
    "fig, ax = plt.subplots(figsize=(15,15))\n",
    "cm_display.plot(ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfTP = [0] * 25\n",
    "listOfFP = [0] * 25\n",
    "listOfTN = [0] * 25\n",
    "listOfFN = [0] * 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(y_test_res)):\n",
    "    y_test_res[i] = g(float(y_test_res[i]))\n",
    "for i in range(0,len(y_res)):\n",
    "    y_res[i] = g(float(y_res[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(listOfTP)): \n",
    "    listOfTP[i], listOfFP[i], listOfTN[i], listOfFN[i] = perf_measure(y_test_res, y_res, y_test_res[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivity = [0] * 25\n",
    "specificity = [0] * 25\n",
    "precision = [0] * 25\n",
    "NPV = [0] * 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sensitivity)): \n",
    "    sensitivity[i], specificity[i], precision[i], NPV[i] = calculate_metrics(listOfTP[i], listOfFP[i], listOfTN[i], listOfFN[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NPV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myset = set(y_test_res)\n",
    "print(myset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mylist = list(myset)\n",
    "print (mylist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfTP"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
